# XGBoost Training Results - ULTRA-SIMPLE MODEL

**Training Date:** 2026-01-09 11:18:54
**Model Type:** Production-ready with ZERO external data needed
**Features:** 5 core features (location + calendar only)
**Removed:** 9 features (all weak or complex-to-predict)
**Hyperparameters:** max_depth=7, n_estimators=200, learning_rate=0.08
**Runtime:** ~5 seconds

---

## Model Configuration

| Parameter | Value |
|-----------|-------|
| max_depth | 7 |
| learning_rate | 0.08 |
| n_estimators | 200 |
| subsample | 0.8 |
| colsample_bytree | 0.8 |
| min_child_weight | 4 |

**Note:** Sample weights used (non-zero blocks weighted 2.5x)

---

## Performance Metrics

### Training Set
| Metric | Value |
|--------|-------|
| RMSE | 4.0425 |
| MAE | 2.9700 |
| RÂ² Score | 0.2436 |

### Test Set
| Metric | Value |
|--------|-------|
| RMSE | 4.0496 |
| MAE | 2.9900 |
| RÂ² Score | 0.2328 |

### Interpretation
- **Average prediction error:** Â±4.05 severity points
- **Typical error:** 2.99 severity points
- **Variance explained:** 23.3%

---

## Feature Importance

| Rank | Feature | Importance | Notes |
|------|---------|------------|-------|
| 1 | Community Area | 0.6427 | ðŸŒŸ Top predictor |
| 2 | weekend_night_peak | 0.1572 | ðŸŒŸ Top predictor |
| 3 | time_block | 0.1354 | ðŸŒŸ Top predictor |
| 4 | month | 0.0355 |  |
| 5 | day_of_week | 0.0292 |  |

---

## Next Steps for Optimization

1. **Remove low-importance features** (importance < 0.01) and retrain
2. **Hyperparameter tuning:** Try different max_depth, learning_rate, n_estimators
3. **Advanced techniques:**
   - Two-stage model (predict zero vs. non-zero, then predict severity)
   - Try LightGBM or CatBoost
   - Add interaction features (e.g., Community_Area Ã— weekend_night_peak)
4. **Temporal validation:** Train on 2023-2024, test on 2025

---

**Files Generated:**
- Model: `01.3_xgboost_model_final.json`
- Report: `01.1_training_results_final.md`
- Plot: `01.2_feature_importance_final.png`